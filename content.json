{"meta":{"title":"黄某人のBlog_(:3」∠)_","subtitle":null,"description":"心之所愿，无所不成","author":"Han$on Huang","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-08-18T10:22:41.000Z","updated":"2019-08-18T10:33:43.904Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"记录分享技术上遇到的困难~Hanson，每天一步一步努力中… 热爱poppin与dota，目前是一枚小小前端人 猪猪の卯，先谢郭嘉，心之所愿，无所不成。"},{"title":"categories","date":"2019-08-18T09:52:21.000Z","updated":"2019-08-18T09:53:36.231Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-08-18T09:57:36.000Z","updated":"2019-08-18T09:57:52.836Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"记录爬虫豆瓣电影数据可视化","slug":"记录爬虫豆瓣电影数据可视化","date":"2019-08-23T01:24:32.000Z","updated":"2019-08-26T13:04:34.278Z","comments":true,"path":"2019/08/23/记录爬虫豆瓣电影数据可视化/","link":"","permalink":"http://yoursite.com/2019/08/23/记录爬虫豆瓣电影数据可视化/","excerpt":"项目计划项目介绍本项目基于node.js，采用 superagent request爬取豆瓣电影数据，存储进MongoDB，最后能通过数据查询，显示影评分和电影数量分析，影评分和电影数量对比分析，近十年每年发行的电影分析，2019各个国家发行的电影数量分析，近十年中国发行的电影评分，近十年中国发行的电影每一年的数量分布图，2019年评分在5分以下的电影名称……利用ECharts进行数据可视化处理","text":"项目计划项目介绍本项目基于node.js，采用 superagent request爬取豆瓣电影数据，存储进MongoDB，最后能通过数据查询，显示影评分和电影数量分析，影评分和电影数量对比分析，近十年每年发行的电影分析，2019各个国家发行的电影数量分析，近十年中国发行的电影评分，近十年中国发行的电影每一年的数量分布图，2019年评分在5分以下的电影名称……利用ECharts进行数据可视化处理 额外说明含有删除线的内容是项目最初采用的技术，因为部分特殊原因（如：无法满足相应的需求，可以进一步优化功能……）而采用其他技术替代 弃用superagnet的原因：①代理ip的不稳定性②部分逻辑功能无法实现（能力有限哈哈）涉及到的内容：无法处理2.1.3，需要优化2.1.2 需求分析 需要什么数据？数据中的directors导演、rate评分、title电影名、国家地区（通过url链接访问再次爬取）？可以在传送数据前先将countries分类确定好获取数据存放MongoDB数据库界面你懂得……按取不同btn会用ECharts显示不同数据 如何获取（从哪里去爬）？豆瓣电影里的 分类 界面中，选择形式：电影 、年代：2019 、国家地区……我们发现只有往下拉点击加载更多的时候页面才会传输数据，加载新的电影信息，所以直接爬取页面数据看来是肯定不行的。那换个方式看：打开f12看Network，加载更多的时候会发现服务器传来一个数据，点开url我们发现这就是我们需要爬取的数据！ 施工过程爬虫开发和优化首先话不多说，先引用需要的模块，随后直接写出superagent的get 1234567891011121314const superagent = require(&apos;superagent&apos;);superagent.get(&quot;https://movie.douban.com/j/new_search_subjects?tags=movie&amp;start=0&amp;year_range=2009,2019&quot;) .end(onresponse);function onresponse(err, data) &#123; if (err) &#123; console.log(err); &#125; else &#123; …… 爬虫 &#125;&#125; 优化后的模板 1234567891011121314151617var options = [], //创建的一个数组，存储所爬取网页的地址 n = 0, //n代表并发请求数，最大值由上面的bfnumber决定 bfnumber = 3 //控制并发生成数的最大值function spider(spider(option, callback) &#123; n++; request(option, function (err, res, data) &#123; if (!err) &#123; …… 爬虫 &#125;else&#123; console.log(err); n--; callback(err, null); &#125; &#125;&#125; 第1坑：数据无法爬下？按照常规来说可以在superagent.get(&#39;目标网页&#39;)后.end()里的回调函数function，可以直接通过 1var $ = cheerio.load(data.text) 根据爬取网页的查看元素，发现需求的数据以json形式存储在&lt;pre&gt;标签之中本应该按如下获取数据 1var sj = $(&apos;pre&apos;).eq(0).text() 实际效果返回的值却为null，这是因为部分大网站的数据是用js输出至页面标签，既然这样无法获取的话，我采取了另外一种办法 12var sj = $(&apos;body&apos;).text();var movieItems = JSON.parse(sj); 这样的效果就是直接获取当前页面数据，在将其转为json数组，成功获取数据 第2坑：ip数据异常被豆瓣限制访问？下述的ip代理方法完全可行，在不考虑2.1.3的情况下（部分ip可能存在质量不高连接不稳定情况），完全足够配合superagent完成爬取工作网站会因为你并发请求数太多当做是在恶意请求，封掉你的ip，为了防止这种情况的发生，这里可以使用一款插件superagent-proxy进行ip代理 1npm install superagent-proxy --save 这里发出superagent-proxy的官方文档，内含详细配置信息设置代理的代理ip，可以选择使用免费的代理服务器帮我们爬数据 （感恩） 1234567891011121314151617require(&apos;superagent-proxy&apos;)(superagent);var proxy = &apos;http://60.205.229.126:80&apos;; //设置代理var header = &#123; &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&apos;, &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.8,zh-TW;q=0.6&apos;, &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Mobile Safari/537.36&apos;, &apos;Cache-Control&apos;: &apos;max-age=0&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;&#125;;superagent.get(&quot;https://movie.douban.com/j/new_search_subjects?tags=movie&amp;start=0&amp;year_range=2009,2019&quot;) .set(&apos;header&apos;, header) .proxy(proxy) .end(onresponse); OK，现在，配置完成后我的index.js内容变为如上。 因为考虑到代理ip的不稳定性，我们采用另外一种方式：使用async.mapLimit控制请求并发网站会因为你并发请求数太多当做是在恶意请求，封掉你的ip，为了防止这种情况的发生，我们一般会在代码里控制并发请求数，Node里面一般借助async模块来实现机缘巧合之下发现了blogNode爬虫之——使用async.mapLimit控制请求并发，在这里有部分内容采取了借鉴 12345678910111213-options：创建的一个数组，存储所爬取网页的地址-bfnumber：控制并发生成数的最大值，这里我取得3-spider：爬虫函数-callback：回调函数，执行完spider后可以在此保存数据async.mapLimit(options, bfnumber, spider.bind(this), function (err, result) &#123; if (err) &#123; console.log(err); &#125; else &#123; …… 数据存储 &#125;&#125;); 第3坑：如何书写逻辑检测数据爬取完毕？ https:// movie.douban.com/j/new_search_subjects?tags=movie&amp; start=0 &amp;year_range=2009,2019 注意这是第一次的数据，其中路由中有个start为0，当start = ?改变后它就会从第?个开始发送20个数据，所以我们首先获取数据就是不断的改变start = startnumber这个数值。这里有个前提就是，豆瓣是利用json每次传输20条电影数据，所以当本次数据长度length &lt; 20时即可说明本次爬取数据结束。那如何做到效果，这成了一个难题？首先想到的是，外面套一个无限循环for循环（无法通过回调函数结束外部循环） 1234for(let startnumber = 0; startnumber &lt; 100; startnumber += 20) &#123; superagent.get(&quot;https://movie.douban.com/j/new_search_subjects?tags=movie&amp;start=&quot; + startnumber + &quot;&amp;year_range=2009,2019&quot;) ……&#125; 面对新的function，尝试进行逐步调试。（推荐的调试方法：先从特殊再到普通） 找特殊情节，例如tags=movie时仅有131条数据，这里既可以判断是否爬取，也可以判断是否length &lt; 20能够结束。当length ≠ 20的时候拒绝callback，然后发现它会从最后1个length &lt; 20的页面，继续爬取bfnumber - 1次数据，获得[]的空白数据。这里当start = 120开始，会继续爬取start = 140,160的数据。所以当length = 0的时候，我们拒绝将该数据存入dataArr。 坑中遇到的疑问 多此一举的保险？（感觉没用）：我甚至为每次进入request请求，var了一个index（正则获取当前url的start后页码/20获得下标），一旦不满足长度=20，便把options数组从当前index+bfnumber后的内容全部清除，避免查询无效页面。此举目的：因为options是最初通过for循环自己添加网址地址，在不清楚后续多少的情况下，直接放入大量（10000/20）条地址。实际效果：能删除，但不n–会抛出err，无法停止，目前仅能通过不再callback来达到停止。↑上诉方法确实没用，今日爬取数据中存在一个问题：部分页面并没有存储20个数据，而下一页将会继续存储，这就导致了提前删除后面存放数据的页面的地址会抛出err，不可取，移除！ 当n = 0后停止，并不会触发外部async.mapLimit的回调函数存储数据，这里添加if(n == 0){……}是为了数据不足内部存储，若正常结束便外部存储 123数据获取后……n--;callback(null, &apos;done!&apos;); 修改过后的代码如下： 1234567891011121314数据获取后……if (length != 0) &#123; dataArr.push(ajson);&#125;n--;if (length == 20) &#123; callback(null, &apos;done!&apos;);&#125; else &#123; options.splice(nowIndex + bfnumber, options.length + 1 - nowIndex - bfnumber) //删除后面的地址数组 if (n == 0) &#123; …… 数据存储 &#125;&#125; 第4坑：豆瓣数据start不能超过9979？存储数据仅有9999条换个思路：一个国家的电影每年肯定不会超过10k部，于是加入&amp;year_range = 20xx,20xx的限制，获取每一年的数据，最后可以将各数据（JSON）拼接起来，获得总的数据。但考虑到项目部分需求的限制，在此我会创建n个数据表存取相应的数据，问题不大。 数据清洗和存储数据清洗OK，通过上述办法我们已经获得了一个json数组，现在进行的是清洗数据，将需求之外的无效数据删除避免占用数据库内存 12var length = movieItems.data.length;movieItems = JSON.stringify(movieItems.data) 这个length的设置就相当有灵性，因为对json字符串利用正则表达式进行数据清洗时，只会进行1次，这时候外面套一个for循环就可以清除掉所有的无效数据，另外length还有一个效果就是进行判定（页面均为每次发送20个电影数据），当length &lt; 20时即可说明本次爬取数据结束，是一个结束的标志。 1234567891011121314for (i = 0; i &lt; length; i++) &#123; var Regstar = /,&quot;(star)&quot;:&quot;(\\d)*&quot;/ movieItems = movieItems.replace(Regstar, &quot;&quot;); var Regcoverx = /,&quot;(cover_x)&quot;:&quot;?(\\d)*&quot;?/ movieItems = movieItems.replace(Regcoverx, &quot;&quot;); var Regcovery = /,&quot;(cover_y)&quot;:&quot;?(\\d)*&quot;?/ movieItems = movieItems.replace(Regcovery, &quot;&quot;); var Regcover = /,&quot;(cover)&quot;:&quot;[\\w:\\/.]*&quot;/ movieItems = movieItems.replace(Regcover, &quot;&quot;); var Regid = /,&quot;(id)&quot;:&quot;(\\d)*&quot;/ movieItems = movieItems.replace(Regid, &quot;&quot;); var Regurl = /,&quot;(url)&quot;:&quot;[\\w:\\/.]*&quot;/ movieItems = movieItems.replace(Regurl, &quot;&quot;);&#125; 数据存储SAVE to JSON先将获取的数据writeFile简单文本写入至/data目录下的json文件中 1234567fs.writeFile(target_dir, JSON.stringify(dataArr), &apos;utf-8&apos;, function (err) &#123; if (err) &#123; console.log(&apos;数据写入失败……&apos;); &#125; else &#123; console.log(&apos;数据写入成功……&apos;); &#125;&#125;) SAVE to MongoDB这里另开一个js专门用来将json数据存储进MongoDB，首先利用fs的readFile简单文本读取。这里的db_name和target_dir分开书写，是为了方便更改读数据和存数据的文件目录。因为有31个数据库，要是嫌麻烦（懒）得话可以简单来个循环，获取文件名字存入数组依次为db_name赋值，这里不做细讲 1234567891011121314var db_name = &apos;2019CN&apos;var target_dir = &apos;data/&apos; + db_name + &apos;.json&apos;;fs.readFile(target_dir, function (err, data) &#123; if (err) throw err; console.log(data) //此时是一个Buffer对象 var savedata = data.toString() console.log(typeof (savedata))//object var dataArr = JSON.parse(savedata) 读数据完毕 ……（存数据）&#125;) 再将其存入数据库sxProduct3中的数据表db_name 123456789101112131415161718192021222324var mongo = require(&quot;mongodb&quot;);var MongoClient = mongo.MongoClient;var url = &quot;mongodb://localhost:27017/&quot;;//测试：先1个再多个nowdata = dataArr[0];MongoClient.connect(url, &#123; useNewUrlParser: true, useUnifiedTopology: true &#125;, function (err, db) &#123; if (err) &#123; console.log(&quot;数据库连接失败...&quot;); &#125; var dbo = db.db(&quot;sxProduct3&quot;); dbo.collection(db_name).insertMany(nowdata, function (err, res) &#123; if (err) &#123; console.log(&quot;数据导入失败...&quot;); throw err; &#125; console.log(&quot;数据插入成功……&quot;); db.close(); &#125;);&#125;); OKOK，我们可以清楚看到数据存储进了数据库，但接下来利用循环存储的时候竟然出现了大问题！ 1234for(var item in dataArr)&#123; nowdata = dataArr[item]; ……&#125; 哈哈，经过最近的沉淀学习可以轻松看出是因为存储的时候是异步执行，数据尚未存储完毕时，nowdata却已重新赋值。OKOK利用上面的思路，给它来之前的控制请求并发，我们给他设置为1次，这样就能如下完美解决。 数据查询和可视化数据查询 12345678910for (var item in dataArr) &#123; console.log(&quot;这是第&quot; + item + &quot;页的数据&quot;) var sb = dataArr[item] for (var sbitme in sb) &#123; if (parseFloat(sb[sbitme].rate) &lt; 8) &#123; console.log(sb[sbitme]) //return;查一个就跳出 &#125; &#125; 其他说明修改记录 19.08.27 修改文档格式规范 19.08.26 添加2.1.4，修改2.2.2 已爬取09-19年各国电影数据，均存入数据库 19.08.24 修改2.1.2，2.1.3 利用async.mapLimit控制请求并发 request代替superagent 19.08.23 创建该文档 参考链接superagent-proxy官方文档Node爬虫之——使用async.mapLimit控制请求并发async.mapLimit 并发请求限制的一点实践","categories":[],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"},{"name":"数据可视化","slug":"数据可视化","permalink":"http://yoursite.com/tags/数据可视化/"},{"name":"ECharts","slug":"ECharts","permalink":"http://yoursite.com/tags/ECharts/"},{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"},{"name":"async","slug":"async","permalink":"http://yoursite.com/tags/async/"}]},{"title":"前端面经复习大纲","slug":"前端面经复习大纲","date":"2019-08-21T01:31:32.000Z","updated":"2019-08-21T01:33:59.494Z","comments":true,"path":"2019/08/21/前端面经复习大纲/","link":"","permalink":"http://yoursite.com/2019/08/21/前端面经复习大纲/","excerpt":"","text":"记录每次面试问题，学会反思","categories":[],"tags":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/tags/前端/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"}]},{"title":"如何往Github上传自己的项目","slug":"如何往Github上传自己的项目","date":"2019-08-18T09:11:45.000Z","updated":"2019-08-18T10:04:26.597Z","comments":true,"path":"2019/08/18/如何往Github上传自己的项目/","link":"","permalink":"http://yoursite.com/2019/08/18/如何往Github上传自己的项目/","excerpt":"Step 1.进入本地目录利用GitBash cd进入当前项目文件地址，可以通过pwd检测","text":"Step 1.进入本地目录利用GitBash cd进入当前项目文件地址，可以通过pwd检测 Step 2.建立git仓库输入git init，成功的标志就是该目录下会多一个.git的隐藏文件夹 Step 3.将文件添加到仓库git add 指定文件名 或git add –all 将所有的文件全部添加 Step 4.将文件提交注释git commit -m “修改说明”，可以通过git status查看状态检查是否成功 Step 5.将本地仓库关联到github并上传12关联：git remote add origin 仓库SSH地址上传：git push -u origin master","categories":[],"tags":[{"name":"新手向","slug":"新手向","permalink":"http://yoursite.com/tags/新手向/"},{"name":"github","slug":"github","permalink":"http://yoursite.com/tags/github/"}]},{"title":"Markdown基本语法","slug":"Markdown基本语法","date":"2019-08-17T02:09:08.000Z","updated":"2019-08-18T10:01:45.771Z","comments":true,"path":"2019/08/17/Markdown基本语法/","link":"","permalink":"http://yoursite.com/2019/08/17/Markdown基本语法/","excerpt":"Markdown是一种纯文本格式的标记语言。通过简单的标记语法，它可以使普通文本内容具有一定的格式。优点：1、因为是纯文本，所以只要支持Markdown的地方都能获得一样的编辑效果，可以让作者摆脱排版的困扰，专心写作。2、操作简单。缺点：1、需要记一些语法（当然，是很简单。五分钟学会）。2、部分平台不支持Markdown编辑模式。","text":"Markdown是一种纯文本格式的标记语言。通过简单的标记语法，它可以使普通文本内容具有一定的格式。优点：1、因为是纯文本，所以只要支持Markdown的地方都能获得一样的编辑效果，可以让作者摆脱排版的困扰，专心写作。2、操作简单。缺点：1、需要记一些语法（当然，是很简单。五分钟学会）。2、部分平台不支持Markdown编辑模式。 一、标题在想要设置为标题的文字前面加#来表示一个#是一级标题，二个#是二级标题，以此类推。支持六级标题。注：标准语法一般在#后跟个空格再写文字，貌似简书不加空格也行。 123456# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题 效果如下： 这是一级标题这是二级标题这是三级标题这是四级标题这是五级标题这是六级标题二、字体 加粗要加粗的文字左右分别用两个*号包起来 斜体要倾斜的文字左右分别用一个*号包起来 斜体加粗要倾斜和加粗的文字左右分别用三个*号包起来 删除线要加删除线的文字左右分别用两个~~号包起来 1234**这是加粗的文字***这是倾斜的文字*`***这是斜体加粗的文字***~~这是加删除线的文字~~ 效果如下：这是加粗的文字这是倾斜的文字`这是斜体加粗的文字这是加删除线的文字 三、引用在引用的文字前加&gt;即可。引用也可以嵌套，如加两个&gt;&gt;三个&gt;&gt;&gt; 123&gt;这是引用的内容&gt;&gt;这是引用的内容&gt;&gt;&gt;&gt;&gt;&gt;&gt;这是引用的内容 效果如下： 这是引用的内容 这是引用的内容 这是引用的内容 四、分割线三个或者三个以上的 - 或者 * 都可以。 1234-------******** 效果如下： 五、图片1![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;) 图片alt就是显示在图片下面的文字，相当于对图片内容的解释。图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加示例： 1![f7a28c0e8391526a6d002ebce8d76841.jpg](https://ww1.yunjiexi.club/2019/08/17/f7a28c0e8391526a6d002ebce8d76841.jpg) 效果如下： 六、超链接1234[超链接名](超链接地址 &quot;超链接title&quot;)title可加可不加[百度](http://baidu.com) 效果如下：百度注：Markdown本身语法不支持链接在新页面中打开，有些平台做了处理，是可以的。如果想要在新页面中打开的话可以用html语言的a标签代替。 123&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;示例&lt;a href=&quot;https://hy530845826.github.io&quot; target=&quot;_blank&quot;&gt;我的blog&lt;/a&gt; 七、列表 无序列表1234无序列表用 - + * 任何一种都可以- 列表内容+ 列表内容* 列表内容 效果如下： 列表内容 列表内容 列表内容 有序列表1234数字加点1. 列表内容2. 列表内容3. 列表内容 效果如下： 列表内容 列表内容 列表内容 列表嵌套上一级和下一级之间敲三个空格即可1234567+ 一级无序列表内容 1. 二级无序列表内容 3. 二级无序列表内容1. 一级无序列表内容 + 二级有序列表内容 + 二级有序列表内容 效果如下： 一级无序列表内容 二级无序列表内容 二级无序列表内容 一级无序列表内容 二级有序列表内容 二级有序列表内容 八、表格12345678910表头|表头|表头- |: - :| -:内容|内容|内容内容|内容|内容第二行分割表头和内容。-文字默认居左-表示文字居中-文字居右-注：原生的语法两边都要用 | 包起来。此处省略 九、代码单行代码：代码之间分别用一个反引号包起来 1`代码内容` 代码块：代码之间分别用三个反引号包起来，且两边的反引号单独占一行 1234567(```) 代码... 代码... 代码...(```)注：为了防止转译，前后三个反引号处加了小括号，实际是没有的。这里只是用来演示，实际中去掉两边小括号即可。 效果如下：单行代码create database hero;代码块 1234function fun()&#123; echo &quot;这是一句非常牛逼的代码&quot;;&#125;fun(); 十、流程图123456789flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op&amp; 效果如下： 原文转自以下链接，自己稍作更正修改。链接：https://www.jianshu.com/p/191d1e21f7ed作者：高鸿祥来源：简书","categories":[],"tags":[{"name":"新手向","slug":"新手向","permalink":"http://yoursite.com/tags/新手向/"},{"name":"markdown","slug":"markdown","permalink":"http://yoursite.com/tags/markdown/"}]},{"title":"next主题上传github.io界面样式无效","slug":"next主题上传github.io界面样式无效","date":"2019-08-16T09:57:25.000Z","updated":"2019-08-18T10:04:09.843Z","comments":true,"path":"2019/08/16/next主题上传github.io界面样式无效/","link":"","permalink":"http://yoursite.com/2019/08/16/next主题上传github.io界面样式无效/","excerpt":"问题描述本地部署后的图片和背景动画，通过hexo d上传至github后，不会加载显示图片和背景动画？","text":"问题描述本地部署后的图片和背景动画，通过hexo d上传至github后，不会加载显示图片和背景动画？ 解决方法尝试性解决？进入/next/source，将其中的lib文件的名字改成其他名字（例：qwe）然后打开/next中的_config.yml，将# Script Vendors.中的_internal: lib修改为_internal: qwe重新在GitBash中输入 123hexo cleanhexo ghexo d 打开博客主页，清理缓存刷新，搞定","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"next主题","slug":"next主题","permalink":"http://yoursite.com/tags/next主题/"}]},{"title":"babel导致webpack打包错误","slug":"babel导致webpack打包错误","date":"2019-08-16T05:37:15.000Z","updated":"2019-08-18T10:03:27.816Z","comments":true,"path":"2019/08/16/babel导致webpack打包错误/","link":"","permalink":"http://yoursite.com/2019/08/16/babel导致webpack打包错误/","excerpt":"问题描述利用npm run bulid进行打包时，抛出Error: Cannot find module ‘@babel/core’&amp;&amp;Cannot find module ‘@babel/plugin-transform-react-jsx’问题发生的首先想到的是webpack.config.js配置环境有误，但反复核对后发现无误观察之中发现视频中的版本的babel-loader之前的版本是@7.1.5,而现在是@8.0.6","text":"问题描述利用npm run bulid进行打包时，抛出Error: Cannot find module ‘@babel/core’&amp;&amp;Cannot find module ‘@babel/plugin-transform-react-jsx’问题发生的首先想到的是webpack.config.js配置环境有误，但反复核对后发现无误观察之中发现视频中的版本的babel-loader之前的版本是@7.1.5,而现在是@8.0.6 解决方法尝试性解决？首先进行尝试性的方法，试试安装@7.1.5版本的babel-loader（回退版本）后会有什么效果 1cnpm i babel-loader@7.1.5 安装完成后，再在终端输入 1npm run bulid 居然完全能运行成功 寻找问题的根源所在？根据官方文档所示：官方默认babel-loader | babel 对应的版本需要一致 解决问题！直接安装最新版本（对应）的babel-loader和babel 1npm install -D babel-loader @babel/core @babel/preset-env webpack 这时候查看jason发现版本匹配，再尝试一下webpack打包，哇塞？成功解决","categories":[],"tags":[{"name":"npm","slug":"npm","permalink":"http://yoursite.com/tags/npm/"},{"name":"webpack","slug":"webpack","permalink":"http://yoursite.com/tags/webpack/"},{"name":"babel","slug":"babel","permalink":"http://yoursite.com/tags/babel/"}]}]}